{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T08:19:30.556486Z",
     "start_time": "2022-12-07T08:19:27.965292Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#模块\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import *\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "import time\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def construct_W(X, **kwargs):\n",
    "\n",
    "    # default metric is 'cosine'\n",
    "    if 'metric' not in kwargs.keys():\n",
    "        kwargs['metric'] = 'cosine'\n",
    "\n",
    "    # default neighbor mode is 'knn' and default neighbor size is 5\n",
    "    if 'neighbor_mode' not in kwargs.keys():\n",
    "        kwargs['neighbor_mode'] = 'knn'\n",
    "    if kwargs['neighbor_mode'] == 'knn' and 'k' not in kwargs.keys():\n",
    "        kwargs['k'] = 5\n",
    "    if kwargs['neighbor_mode'] == 'supervised' and 'k' not in kwargs.keys():\n",
    "        kwargs['k'] = 5\n",
    "    if kwargs['neighbor_mode'] == 'supervised' and 'y' not in kwargs.keys():\n",
    "        print ('Warning: label is required in the supervised neighborMode!!!')\n",
    "        exit(0)\n",
    "\n",
    "    # default weight mode is 'binary', default t in heat kernel mode is 1\n",
    "    if 'weight_mode' not in kwargs.keys():\n",
    "        kwargs['weight_mode'] = 'binary'\n",
    "    if kwargs['weight_mode'] == 'heat_kernel':\n",
    "        if kwargs['metric'] != 'euclidean':\n",
    "            kwargs['metric'] = 'euclidean'\n",
    "        if 't' not in kwargs.keys():\n",
    "            kwargs['t'] = 1\n",
    "    elif kwargs['weight_mode'] == 'cosine':\n",
    "        if kwargs['metric'] != 'cosine':\n",
    "            kwargs['metric'] = 'cosine'\n",
    "\n",
    "    # default fisher_score and reliefF mode are 'false'\n",
    "    if 'fisher_score' not in kwargs.keys():\n",
    "        kwargs['fisher_score'] = False\n",
    "    if 'reliefF' not in kwargs.keys():\n",
    "        kwargs['reliefF'] = False\n",
    "\n",
    "    n_samples, n_features = np.shape(X)\n",
    "\n",
    "    # choose 'knn' neighbor mode\n",
    "    if kwargs['neighbor_mode'] == 'knn':\n",
    "        k = kwargs['k']\n",
    "        if kwargs['weight_mode'] == 'binary':\n",
    "            if kwargs['metric'] == 'euclidean':\n",
    "                # compute pairwise euclidean distances\n",
    "                D = pairwise_distances(X)\n",
    "                D **= 2\n",
    "                # sort the distance matrix D in ascending order\n",
    "                dump = np.sort(D, axis=1)\n",
    "                idx = np.argsort(D, axis=1)\n",
    "                # choose the k-nearest neighbors for each instance\n",
    "                idx_new = idx[:, 0:k+1]\n",
    "                G = np.zeros((n_samples*(k+1), 3))\n",
    "                G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n",
    "                G[:, 1] = np.ravel(idx_new, order='F')\n",
    "                G[:, 2] = 1\n",
    "                # build the sparse affinity matrix W\n",
    "                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "                bigger = np.transpose(W) > W\n",
    "                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "                return W\n",
    "\n",
    "            elif kwargs['metric'] == 'cosine':\n",
    "                # normalize the data first\n",
    "                X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n",
    "                for i in range(n_samples):\n",
    "                    X[i, :] = X[i, :]/max(1e-12, X_normalized[i])\n",
    "                # compute pairwise cosine distances\n",
    "                D_cosine = np.dot(X, np.transpose(X))\n",
    "                # sort the distance matrix D in descending order\n",
    "                dump = np.sort(-D_cosine, axis=1)\n",
    "                idx = np.argsort(-D_cosine, axis=1)\n",
    "                idx_new = idx[:, 0:k+1]\n",
    "                G = np.zeros((n_samples*(k+1), 3))\n",
    "                G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n",
    "                G[:, 1] = np.ravel(idx_new, order='F')\n",
    "                G[:, 2] = 1\n",
    "                # build the sparse affinity matrix W\n",
    "                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "                bigger = np.transpose(W) > W\n",
    "                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "                return W\n",
    "\n",
    "        elif kwargs['weight_mode'] == 'heat_kernel':\n",
    "            t = kwargs['t']\n",
    "            # compute pairwise euclidean distances\n",
    "            D = pairwise_distances(X)\n",
    "            D **= 2\n",
    "            # sort the distance matrix D in ascending order\n",
    "            dump = np.sort(D, axis=1)\n",
    "            idx = np.argsort(D, axis=1)\n",
    "            idx_new = idx[:, 0:k+1]\n",
    "            dump_new = dump[:, 0:k+1]\n",
    "            # compute the pairwise heat kernel distances\n",
    "            dump_heat_kernel = np.exp(-dump_new/(2*t*t))\n",
    "            G = np.zeros((n_samples*(k+1), 3))\n",
    "            G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n",
    "            G[:, 1] = np.ravel(idx_new, order='F')\n",
    "            G[:, 2] = np.ravel(dump_heat_kernel, order='F')\n",
    "            # build the sparse affinity matrix W\n",
    "            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            bigger = np.transpose(W) > W\n",
    "            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "            return W\n",
    "\n",
    "        elif kwargs['weight_mode'] == 'cosine':\n",
    "            # normalize the data first\n",
    "            X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n",
    "            for i in range(n_samples):\n",
    "                    X[i, :] = X[i, :]/max(1e-12, X_normalized[i])\n",
    "            # compute pairwise cosine distances\n",
    "            D_cosine = np.dot(X, np.transpose(X))\n",
    "            # sort the distance matrix D in ascending order\n",
    "            dump = np.sort(-D_cosine, axis=1)\n",
    "            idx = np.argsort(-D_cosine, axis=1)\n",
    "            idx_new = idx[:, 0:k+1]\n",
    "            dump_new = -dump[:, 0:k+1]\n",
    "            G = np.zeros((n_samples*(k+1), 3))\n",
    "            G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n",
    "            G[:, 1] = np.ravel(idx_new, order='F')\n",
    "            G[:, 2] = np.ravel(dump_new, order='F')\n",
    "            # build the sparse affinity matrix W\n",
    "            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            bigger = np.transpose(W) > W\n",
    "            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "            return W\n",
    "\n",
    "    # choose supervised neighborMode\n",
    "    elif kwargs['neighbor_mode'] == 'supervised':\n",
    "        k = kwargs['k']\n",
    "        # get true labels and the number of classes\n",
    "        y = kwargs['y']\n",
    "        label = np.unique(y)\n",
    "        n_classes = np.unique(y).size\n",
    "        # construct the weight matrix W in a fisherScore way, W_ij = 1/n_l if yi = yj = l, otherwise W_ij = 0\n",
    "        if kwargs['fisher_score'] is True:\n",
    "            W = lil_matrix((n_samples, n_samples))\n",
    "            for i in range(n_classes):\n",
    "                class_idx = (y == label[i])\n",
    "                class_idx_all = (class_idx[:, np.newaxis] & class_idx[np.newaxis, :])\n",
    "                W[class_idx_all] = 1.0/np.sum(np.sum(class_idx))\n",
    "            return W\n",
    "\n",
    "        # construct the weight matrix W in a reliefF way, NH(x) and NM(x,y) denotes a set of k nearest\n",
    "        # points to x with the same class as x, a different class (the class y), respectively. W_ij = 1 if i = j;\n",
    "        # W_ij = 1/k if x_j \\in NH(x_i); W_ij = -1/(c-1)k if x_j \\in NM(x_i, y)\n",
    "        if kwargs['reliefF'] is True:\n",
    "            # when xj in NH(xi)\n",
    "            G = np.zeros((n_samples*(k+1), 3))\n",
    "            id_now = 0\n",
    "            for i in range(n_classes):\n",
    "                class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                D = pairwise_distances(X[class_idx, :])\n",
    "                D **= 2\n",
    "                idx = np.argsort(D, axis=1)\n",
    "                idx_new = idx[:, 0:k+1]\n",
    "                n_smp_class = (class_idx[idx_new[:]]).size\n",
    "                if len(class_idx) <= k:\n",
    "                    k = len(class_idx) - 1\n",
    "                G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n",
    "                G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n",
    "                G[id_now:n_smp_class+id_now, 2] = 1.0/k\n",
    "                id_now += n_smp_class\n",
    "            W1 = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            # when i = j, W_ij = 1\n",
    "            for i in range(n_samples):\n",
    "                W1[i, i] = 1\n",
    "            # when x_j in NM(x_i, y)\n",
    "            G = np.zeros((n_samples*k*(n_classes - 1), 3))\n",
    "            id_now = 0\n",
    "            for i in range(n_classes):\n",
    "                class_idx1 = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                X1 = X[class_idx1, :]\n",
    "                for j in range(n_classes):\n",
    "                    if label[j] != label[i]:\n",
    "                        class_idx2 = np.column_stack(np.where(y == label[j]))[:, 0]\n",
    "                        X2 = X[class_idx2, :]\n",
    "                        D = pairwise_distances(X1, X2)\n",
    "                        idx = np.argsort(D, axis=1)\n",
    "                        idx_new = idx[:, 0:k]\n",
    "                        n_smp_class = len(class_idx1)*k\n",
    "                        G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx1, (k, 1)).reshape(-1)\n",
    "                        G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx2[idx_new[:]], order='F')\n",
    "                        G[id_now:n_smp_class+id_now, 2] = -1.0/((n_classes-1)*k)\n",
    "                        id_now += n_smp_class\n",
    "            W2 = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            bigger = np.transpose(W2) > W2\n",
    "            W2 = W2 - W2.multiply(bigger) + np.transpose(W2).multiply(bigger)\n",
    "            W = W1 + W2\n",
    "            return W\n",
    "\n",
    "        if kwargs['weight_mode'] == 'binary':\n",
    "            if kwargs['metric'] == 'euclidean':\n",
    "                G = np.zeros((n_samples*(k+1), 3))\n",
    "                id_now = 0\n",
    "                for i in range(n_classes):\n",
    "                    class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                    # compute pairwise euclidean distances for instances in class i\n",
    "                    D = pairwise_distances(X[class_idx, :])\n",
    "                    D **= 2\n",
    "                    # sort the distance matrix D in ascending order for instances in class i\n",
    "                    idx = np.argsort(D, axis=1)\n",
    "                    idx_new = idx[:, 0:k+1]\n",
    "                    n_smp_class = len(class_idx)*(k+1)\n",
    "                    G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n",
    "                    G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n",
    "                    G[id_now:n_smp_class+id_now, 2] = 1\n",
    "                    id_now += n_smp_class\n",
    "                # build the sparse affinity matrix W\n",
    "                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "                bigger = np.transpose(W) > W\n",
    "                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "                return W\n",
    "\n",
    "            if kwargs['metric'] == 'cosine':\n",
    "                # normalize the data first\n",
    "                X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n",
    "                for i in range(n_samples):\n",
    "                    X[i, :] = X[i, :]/max(1e-12, X_normalized[i])\n",
    "                G = np.zeros((n_samples*(k+1), 3))\n",
    "                id_now = 0\n",
    "                for i in range(n_classes):\n",
    "                    class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                    # compute pairwise cosine distances for instances in class i\n",
    "                    D_cosine = np.dot(X[class_idx, :], np.transpose(X[class_idx, :]))\n",
    "                    # sort the distance matrix D in descending order for instances in class i\n",
    "                    idx = np.argsort(-D_cosine, axis=1)\n",
    "                    idx_new = idx[:, 0:k+1]\n",
    "                    n_smp_class = len(class_idx)*(k+1)\n",
    "                    G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n",
    "                    G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n",
    "                    G[id_now:n_smp_class+id_now, 2] = 1\n",
    "                    id_now += n_smp_class\n",
    "                # build the sparse affinity matrix W\n",
    "                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "                bigger = np.transpose(W) > W\n",
    "                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "                return W\n",
    "\n",
    "        elif kwargs['weight_mode'] == 'heat_kernel':\n",
    "            G = np.zeros((n_samples*(k+1), 3))\n",
    "            id_now = 0\n",
    "            for i in range(n_classes):\n",
    "                class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                # compute pairwise cosine distances for instances in class i\n",
    "                D = pairwise_distances(X[class_idx, :])\n",
    "                D **= 2\n",
    "                # sort the distance matrix D in ascending order for instances in class i\n",
    "                dump = np.sort(D, axis=1)\n",
    "                idx = np.argsort(D, axis=1)\n",
    "                idx_new = idx[:, 0:k+1]\n",
    "                dump_new = dump[:, 0:k+1]\n",
    "                t = kwargs['t']\n",
    "                # compute pairwise heat kernel distances for instances in class i\n",
    "                dump_heat_kernel = np.exp(-dump_new/(2*t*t))\n",
    "                n_smp_class = len(class_idx)*(k+1)\n",
    "                G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n",
    "                G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n",
    "                G[id_now:n_smp_class+id_now, 2] = np.ravel(dump_heat_kernel, order='F')\n",
    "                id_now += n_smp_class\n",
    "            # build the sparse affinity matrix W\n",
    "            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            bigger = np.transpose(W) > W\n",
    "            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "            return W\n",
    "\n",
    "        elif kwargs['weight_mode'] == 'cosine':\n",
    "            # normalize the data first\n",
    "            X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n",
    "            for i in range(n_samples):\n",
    "                X[i, :] = X[i, :]/max(1e-12, X_normalized[i])\n",
    "            G = np.zeros((n_samples*(k+1), 3))\n",
    "            id_now = 0\n",
    "            for i in range(n_classes):\n",
    "                class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                # compute pairwise cosine distances for instances in class i\n",
    "                D_cosine = np.dot(X[class_idx, :], np.transpose(X[class_idx, :]))\n",
    "                # sort the distance matrix D in descending order for instances in class i\n",
    "                dump = np.sort(-D_cosine, axis=1)\n",
    "                idx = np.argsort(-D_cosine, axis=1)\n",
    "                idx_new = idx[:, 0:k+1]\n",
    "                dump_new = -dump[:, 0:k+1]\n",
    "                n_smp_class = len(class_idx)*(k+1)\n",
    "                G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n",
    "                G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n",
    "                G[id_now:n_smp_class+id_now, 2] = np.ravel(dump_new, order='F')\n",
    "                id_now += n_smp_class\n",
    "            # build the sparse affinity matrix W\n",
    "            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            bigger = np.transpose(W) > W\n",
    "            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "            return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T10:35:38.352661Z",
     "start_time": "2022-12-07T10:35:38.330738Z"
    },
    "code_folding": [
     0,
     4,
     18,
     44,
     52,
     64,
     70
    ]
   },
   "outputs": [],
   "source": [
    "#函数\n",
    "import numpy as np\n",
    "from scipy.sparse import *\n",
    "res = pd.DataFrame(0,columns=[\"spnsi\",\"speci\",\"GM\",\"f1\",\"AUC\"],index=dataList)\n",
    "def fisher_score(X, y):\n",
    "\n",
    "    # Construct weight matrix W in a fisherScore way\n",
    "    kwargs = {\"neighbor_mode\": \"supervised\", \"fisher_score\": True, 'y': y}\n",
    "    W = construct_W(X, **kwargs)\n",
    "\n",
    "    # build the diagonal D matrix from affinity matrix W\n",
    "    D = np.array(W.sum(axis=1))\n",
    "    L = W\n",
    "    tmp = np.dot(np.transpose(D), X)\n",
    "    D = diags(np.transpose(D), [0])\n",
    "    Xt = np.transpose(X)\n",
    "    t1 = np.transpose(np.dot(Xt, D.todense()))\n",
    "    t2 = np.transpose(np.dot(Xt, L.todense()))\n",
    "    # compute the numerator of Lr\n",
    "    D_prime = np.sum(np.multiply(t1, X), 0) - np.multiply(tmp, tmp)/D.sum()\n",
    "    # compute the denominator of Lr\n",
    "    L_prime = np.sum(np.multiply(t2, X), 0) - np.multiply(tmp, tmp)/D.sum()\n",
    "    # avoid the denominator of Lr to be 0\n",
    "    D_prime[D_prime < 1e-12] = 10000\n",
    "    lap_score = 1 - np.array(np.multiply(L_prime, 1/D_prime))[0, :]\n",
    "\n",
    "    # compute fisher score from laplacian score, where fisher_score = 1/lap_score - 1\n",
    "    score = 1.0/lap_score - 1\n",
    "    return np.transpose(score)\n",
    "\n",
    "def feature_ranking(score):\n",
    "    \"\"\"\n",
    "    Rank features in descending order according to fisher score, the larger the fisher score, the more important the\n",
    "    feature is\n",
    "    \"\"\"\n",
    "    idx = np.argsort(score, 0)\n",
    "    return idx[::-1]\n",
    "\n",
    "def GM_score(x,y): #计算GM 传入numpy\n",
    "    cm1 = confusion_matrix(x,y)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    return sensitivity1,specificity1\n",
    "\n",
    "def getAncestors(feat):\n",
    "    global relativeList\n",
    "    for relative in DAG.columns:\n",
    "        if(DAG.loc[relative,feat] == 1):\n",
    "            relativeList.append(relative)\n",
    "            getAncestors(relative)\n",
    "def getDescendants(feat):\n",
    "    global relativeList\n",
    "    for relative in DAG.columns:\n",
    "        if(DAG.loc[feat,relative] == 1):\n",
    "            relativeList.append(relative)\n",
    "            getAncestors(relative)\n",
    "def get(feat):\n",
    "    global relativeList\n",
    "    corr = []\n",
    "    #先找到当前节点的所有祖先或后代节点\n",
    "    relativeList = []\n",
    "    getDescendants(feat)\n",
    "    getAncestors(feat)\n",
    "    relativeSet = set(relativeList)\n",
    "    for relative in relativeSet:\n",
    "        corr.append(metrics.normalized_mutual_info_score(data.loc[:,c1],data.loc[:,c2]))\n",
    "    #print(corr)\n",
    "    if(corr == []):\n",
    "        return 0\n",
    "    else:\n",
    "        return np.mean(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T08:19:26.947368Z",
     "start_time": "2022-12-07T08:19:26.937362Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataList = [\"MM/MM-BP.csv\",\"MM/MM-CC.csv\",\"MM/MM-MF.csv\",\"MM/MM-BPCC.csv\",\n",
    "#             \"MM/MM-BPMF.csv\",\"MM/MM-CCMF.csv\",\"MM/MM-BPCCMF.csv\",\n",
    "#            \"CE/CE-BP.csv\",\"CE/CE-CC.csv\",\"CE/CE-MF.csv\",\"CE/CE-BPCC.csv\",\n",
    "#             \"CE/CE-BPMF.csv\",\"CE/CE-CCMF.csv\",\"CE/CE-BPCCMF.csv\",\n",
    "#            \"DM/DM-BP.csv\",\"DM/DM-CC.csv\",\"DM/DM-MF.csv\",\"DM/DM-BPCC.csv\",\n",
    "#             \"DM/DM-BPMF.csv\",\"DM/DM-CCMF.csv\",\"DM/DM-BPCCMF.csv\",\n",
    "#            \"SC/SC-BP.csv\",\"SC/SC-CC.csv\",\"SC/SC-MF.csv\",\"SC/SC-BPCC.csv\",\n",
    "#             \"SC/SC-BPMF.csv\",\"SC/SC-CCMF.csv\",\"SC/SC-BPCCMF.csv\"]\n",
    "dataList = [\"Cities.csv\",\"NYDailyheadings.csv\",\"SportsTweetsc.csv\",\"SportsTweetst.csv\",\"Stumbleupon.csv\"]\n",
    "dataName = \"SportsTweetst.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T11:14:17.267234Z",
     "start_time": "2022-12-07T11:13:51.297586Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#加载数据 预处理\n",
    "time_start = time.time()  # 记录开始时间\n",
    "data = pd.read_csv(\"../data/real_world_datasets/Datasets/\"+dataName,index_col=0)\n",
    "if(dataName == \"Cities.csv\"):\n",
    "    data[\"longevity influence\"] = data[\"longevity influence\"].apply(lambda x : 1 if x > 1.751738e+09 else 0)\n",
    "label = data[\"longevity influence\"]\n",
    "#删除无用列\n",
    "data = data.drop([\"longevity influence\"],axis=1)\n",
    "#过滤低纬度特征\n",
    "for c in data.columns:\n",
    "    data[c] = data[c].astype('int')\n",
    "    if(data[c].sum() < 3):\n",
    "        data.drop(c,axis=1,inplace=True)\n",
    "DAG = pd.read_csv(\"../data/real_world_datasets/Hierarchy/\"+dataName,index_col=0)\n",
    "DAG = DAG.loc[data.columns,data.columns]\n",
    "labels_pred=data.values.tolist()\n",
    "labels_true=label.values.tolist()\n",
    "#读取计算结果\n",
    "hierarchicalR = pd.read_csv(\"../\"+dataName,index_col=0)\n",
    "# Relevance = pd.Series(fisher_score(labels_pred,labels_true),index=data.columns)\n",
    "#α惩罚项\n",
    "# SUMetrics = pd.DataFrame(0,columns=data.columns,index=data.columns)\n",
    "# for c1 in data.columns:\n",
    "#     print(c1)\n",
    "#     for c2 in data.columns:\n",
    "#         if(c1 != c2):\n",
    "#             SUMetrics.loc[c1,c2] = metrics.normalized_mutual_info_score(data.loc[:,c1],data.loc[:,c2])\n",
    "\n",
    "# relativeList = []\n",
    "# global relativeList\n",
    "# avgCorr = pd.Series(0,index=data.columns)\n",
    "# for c in data.columns:\n",
    "#     print(1)\n",
    "#     avgCorr.loc[c] = get(c)\n",
    "\n",
    "#β惩罚项\n",
    "\n",
    "# relativeList = []\n",
    "# global relativeList\n",
    "# countAncestors = pd.Series(0,index=data.columns)\n",
    "# for c in data.columns[4:]:\n",
    "#     print(1)\n",
    "#     relativeList = []\n",
    "#     getAncestors(c) \n",
    "#     countAncestors.loc[c] = len(set(relativeList))\n",
    "\n",
    "#归一化\n",
    "# Relevance = (Relevance - Relevance.min()) / (Relevance.max() - Relevance.min())\n",
    "# avgCorr = (avgCorr - avgCorr.min()) / (avgCorr.max() - avgCorr.min())\n",
    "# countAncestors = (countAncestors - countAncestors.min()) / (countAncestors.max() - countAncestors.min())\n",
    "# #带层次冗余惩罚的相关性系数\n",
    "# alpha = 0.1\n",
    "# beta = 0.2\n",
    "#hierarchicalR = Relevance - alpha * avgCorr - beta * countAncestors\n",
    "#hierarchicalR = hierarchicalR.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T11:14:20.403180Z",
     "start_time": "2022-12-07T11:14:19.798446Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 1\n",
      " 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1\n",
      " 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1\n",
      " 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
      " 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1\n",
      " 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1\n",
      " 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0\n",
      " 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0\n",
      " 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1\n",
      " 0 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "[0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0.7456072599192959\n",
      "0.8716325237285426\n",
      "GM\n",
      "80.6\n",
      "f1\n",
      "71.4\n",
      "AUC\n",
      "80.9\n",
      "14.586827278137207\n"
     ]
    }
   ],
   "source": [
    "#执行\n",
    "num = int(data.shape[1]*0.7)\n",
    "sensitivity = []\n",
    "specificity = []\n",
    "F1 = []\n",
    "AUC = []\n",
    "numList = []\n",
    "#10折交叉验证\n",
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "for train_index ,test_index in kf.split(data):\n",
    "    trainData = data.iloc[train_index,:]\n",
    "    testData = data.iloc[test_index,:]\n",
    "    Y_train = label.values[train_index] #用于训练模型\n",
    "    Y_test = label.values[test_index]#用于交叉验证\n",
    "\n",
    "    selectFeatures = hierarchicalR[:num].index#CFS\n",
    "    #selectFeatures = data.columns\n",
    "\n",
    "    X_train = trainData.loc[:,selectFeatures]\n",
    "    X_test = testData.loc[:,selectFeatures]\n",
    "#     gnb = GaussianNB()\n",
    "#     predictList = gnb.fit(X_train, Y_train).predict(X_test)\n",
    "#     from sklearn.svm import SVC\n",
    "#     clf = make_pipeline(StandardScaler(), SVC(gamma=0.5))\n",
    "#     clf.fit(X_train, Y_train)\n",
    "#     predictList = clf.predict(X_test)\n",
    "    rfc = RandomForestClassifier(random_state=0)\n",
    "    rfc = rfc.fit(X_train,Y_train)\n",
    "    predictList = rfc.predict(X_test)\n",
    "    print(predictList)\n",
    "    print(Y_test)\n",
    "    sensi,speci= GM_score(predictList,Y_test)\n",
    "    sensitivity.append(sensi)\n",
    "    specificity.append(speci)\n",
    "    F1.append(f1_score(np.array(predictList),Y_test))\n",
    "    try:\n",
    "        AUC.append(roc_auc_score(np.array(predictList),Y_test))\n",
    "    except ValueError:\n",
    "        pass\n",
    "#standard\n",
    "a = np.nanmean(sensitivity)\n",
    "b = np.nanmean(specificity)\n",
    "print(a)\n",
    "print(b)\n",
    "print(\"GM\")\n",
    "print(round(math.sqrt(a*b)*100,1))\n",
    "print(\"f1\")\n",
    "print(round(np.nanmean(F1)*100,1))\n",
    "print(\"AUC\") \n",
    "print(round(np.nanmean(AUC)*100,1))\n",
    "res.loc[dataName,\"spnsi\"] = round(a*100,1)\n",
    "res.loc[dataName,\"speci\"] = round(b*100,1)\n",
    "res.loc[dataName,\"GM\"] = round(math.sqrt(a*b)*100,1)\n",
    "res.loc[dataName,\"f1\"] = round(np.nanmean(F1)*100,1)\n",
    "res.loc[dataName,\"AUC\"] = round(np.nanmean(AUC)*100,1)\n",
    "\n",
    "time_end = time.time()  # 记录结束时间\n",
    "time_sum = time_end - time_start  # 计算的时间差为程序的执行时间，单位为秒/s\n",
    "print(time_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T11:16:28.008761Z",
     "start_time": "2022-12-07T11:16:28.001729Z"
    }
   },
   "outputs": [],
   "source": [
    "res.to_csv(\"../res.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2813,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.read_csv(\"data/real_world_datasets/Datasets/\"+dataName,index_col=0)[\"longevity influence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2815,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label.apply(lambda x : 0 if x < 1e+09 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    173\n",
       "0     39\n",
       "Name: longevity influence, dtype: int64"
      ]
     },
     "execution_count": 2816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T10:39:40.338403Z",
     "start_time": "2022-12-07T10:39:40.305729Z"
    }
   },
   "outputs": [],
   "source": [
    "hierarchicalR = hierarchicalR-hierarchicalR.mean()\n",
    "hierarchicalR = hierarchicalR.sort_values(ascending = False)\n",
    "hierarchicalR.to_csv(\"../Tweetsc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T10:39:13.180743Z",
     "start_time": "2022-12-07T10:39:13.162172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0077138798539807055"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierarchicalR.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "628.85px",
    "left": "1601px",
    "right": "20px",
    "top": "213px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
