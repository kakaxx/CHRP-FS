{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T11:47:42.288522Z",
     "start_time": "2022-12-07T11:47:42.207570Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#模块\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import *\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "import time\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def construct_W(X, **kwargs):\n",
    "    \"\"\"\n",
    "    Construct the affinity matrix W through different ways\n",
    "    Notes\n",
    "    -----\n",
    "    if kwargs is null, use the default parameter settings;\n",
    "    if kwargs is not null, construct the affinity matrix according to parameters in kwargs\n",
    "    Input\n",
    "    -----\n",
    "    X: {numpy array}, shape (n_samples, n_features)\n",
    "        input data\n",
    "    kwargs: {dictionary}\n",
    "        parameters to construct different affinity matrix W:\n",
    "        y: {numpy array}, shape (n_samples, 1)\n",
    "            the true label information needed under the 'supervised' neighbor mode\n",
    "        metric: {string}\n",
    "            choices for different distance measures\n",
    "            'euclidean' - use euclidean distance\n",
    "            'cosine' - use cosine distance (default)\n",
    "        neighbor_mode: {string}\n",
    "            indicates how to construct the graph\n",
    "            'knn' - put an edge between two nodes if and only if they are among the\n",
    "                    k nearest neighbors of each other (default)\n",
    "            'supervised' - put an edge between two nodes if they belong to same class\n",
    "                    and they are among the k nearest neighbors of each other\n",
    "        weight_mode: {string}\n",
    "            indicates how to assign weights for each edge in the graph\n",
    "            'binary' - 0-1 weighting, every edge receives weight of 1 (default)\n",
    "            'heat_kernel' - if nodes i and j are connected, put weight W_ij = exp(-norm(x_i - x_j)/2t^2)\n",
    "                            this weight mode can only be used under 'euclidean' metric and you are required\n",
    "                            to provide the parameter t\n",
    "            'cosine' - if nodes i and j are connected, put weight cosine(x_i,x_j).\n",
    "                        this weight mode can only be used under 'cosine' metric\n",
    "        k: {int}\n",
    "            choices for the number of neighbors (default k = 5)\n",
    "        t: {float}\n",
    "            parameter for the 'heat_kernel' weight_mode\n",
    "        fisher_score: {boolean}\n",
    "            indicates whether to build the affinity matrix in a fisher score way, in which W_ij = 1/n_l if yi = yj = l;\n",
    "            otherwise W_ij = 0 (default fisher_score = false)\n",
    "        reliefF: {boolean}\n",
    "            indicates whether to build the affinity matrix in a reliefF way, NH(x) and NM(x,y) denotes a set of\n",
    "            k nearest points to x with the same class as x, and a different class (the class y), respectively.\n",
    "            W_ij = 1 if i = j; W_ij = 1/k if x_j \\in NH(x_i); W_ij = -1/(c-1)k if x_j \\in NM(x_i, y) (default reliefF = false)\n",
    "    Output\n",
    "    ------\n",
    "    W: {sparse matrix}, shape (n_samples, n_samples)\n",
    "        output affinity matrix W\n",
    "    \"\"\"\n",
    "\n",
    "    # default metric is 'cosine'\n",
    "    if 'metric' not in kwargs.keys():\n",
    "        kwargs['metric'] = 'cosine'\n",
    "\n",
    "    # default neighbor mode is 'knn' and default neighbor size is 5\n",
    "    if 'neighbor_mode' not in kwargs.keys():\n",
    "        kwargs['neighbor_mode'] = 'knn'\n",
    "    if kwargs['neighbor_mode'] == 'knn' and 'k' not in kwargs.keys():\n",
    "        kwargs['k'] = 5\n",
    "    if kwargs['neighbor_mode'] == 'supervised' and 'k' not in kwargs.keys():\n",
    "        kwargs['k'] = 5\n",
    "    if kwargs['neighbor_mode'] == 'supervised' and 'y' not in kwargs.keys():\n",
    "        print ('Warning: label is required in the supervised neighborMode!!!')\n",
    "        exit(0)\n",
    "\n",
    "    # default weight mode is 'binary', default t in heat kernel mode is 1\n",
    "    if 'weight_mode' not in kwargs.keys():\n",
    "        kwargs['weight_mode'] = 'binary'\n",
    "    if kwargs['weight_mode'] == 'heat_kernel':\n",
    "        if kwargs['metric'] != 'euclidean':\n",
    "            kwargs['metric'] = 'euclidean'\n",
    "        if 't' not in kwargs.keys():\n",
    "            kwargs['t'] = 1\n",
    "    elif kwargs['weight_mode'] == 'cosine':\n",
    "        if kwargs['metric'] != 'cosine':\n",
    "            kwargs['metric'] = 'cosine'\n",
    "\n",
    "    # default fisher_score and reliefF mode are 'false'\n",
    "    if 'fisher_score' not in kwargs.keys():\n",
    "        kwargs['fisher_score'] = False\n",
    "    if 'reliefF' not in kwargs.keys():\n",
    "        kwargs['reliefF'] = False\n",
    "\n",
    "    n_samples, n_features = np.shape(X)\n",
    "\n",
    "    # choose 'knn' neighbor mode\n",
    "    if kwargs['neighbor_mode'] == 'knn':\n",
    "        k = kwargs['k']\n",
    "        if kwargs['weight_mode'] == 'binary':\n",
    "            if kwargs['metric'] == 'euclidean':\n",
    "                # compute pairwise euclidean distances\n",
    "                D = pairwise_distances(X)\n",
    "                D **= 2\n",
    "                # sort the distance matrix D in ascending order\n",
    "                dump = np.sort(D, axis=1)\n",
    "                idx = np.argsort(D, axis=1)\n",
    "                # choose the k-nearest neighbors for each instance\n",
    "                idx_new = idx[:, 0:k+1]\n",
    "                G = np.zeros((n_samples*(k+1), 3))\n",
    "                G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n",
    "                G[:, 1] = np.ravel(idx_new, order='F')\n",
    "                G[:, 2] = 1\n",
    "                # build the sparse affinity matrix W\n",
    "                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "                bigger = np.transpose(W) > W\n",
    "                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "                return W\n",
    "\n",
    "            elif kwargs['metric'] == 'cosine':\n",
    "                # normalize the data first\n",
    "                X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n",
    "                for i in range(n_samples):\n",
    "                    X[i, :] = X[i, :]/max(1e-12, X_normalized[i])\n",
    "                # compute pairwise cosine distances\n",
    "                D_cosine = np.dot(X, np.transpose(X))\n",
    "                # sort the distance matrix D in descending order\n",
    "                dump = np.sort(-D_cosine, axis=1)\n",
    "                idx = np.argsort(-D_cosine, axis=1)\n",
    "                idx_new = idx[:, 0:k+1]\n",
    "                G = np.zeros((n_samples*(k+1), 3))\n",
    "                G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n",
    "                G[:, 1] = np.ravel(idx_new, order='F')\n",
    "                G[:, 2] = 1\n",
    "                # build the sparse affinity matrix W\n",
    "                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "                bigger = np.transpose(W) > W\n",
    "                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "                return W\n",
    "\n",
    "        elif kwargs['weight_mode'] == 'heat_kernel':\n",
    "            t = kwargs['t']\n",
    "            # compute pairwise euclidean distances\n",
    "            D = pairwise_distances(X)\n",
    "            D **= 2\n",
    "            # sort the distance matrix D in ascending order\n",
    "            dump = np.sort(D, axis=1)\n",
    "            idx = np.argsort(D, axis=1)\n",
    "            idx_new = idx[:, 0:k+1]\n",
    "            dump_new = dump[:, 0:k+1]\n",
    "            # compute the pairwise heat kernel distances\n",
    "            dump_heat_kernel = np.exp(-dump_new/(2*t*t))\n",
    "            G = np.zeros((n_samples*(k+1), 3))\n",
    "            G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n",
    "            G[:, 1] = np.ravel(idx_new, order='F')\n",
    "            G[:, 2] = np.ravel(dump_heat_kernel, order='F')\n",
    "            # build the sparse affinity matrix W\n",
    "            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            bigger = np.transpose(W) > W\n",
    "            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "            return W\n",
    "\n",
    "        elif kwargs['weight_mode'] == 'cosine':\n",
    "            # normalize the data first\n",
    "            X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n",
    "            for i in range(n_samples):\n",
    "                    X[i, :] = X[i, :]/max(1e-12, X_normalized[i])\n",
    "            # compute pairwise cosine distances\n",
    "            D_cosine = np.dot(X, np.transpose(X))\n",
    "            # sort the distance matrix D in ascending order\n",
    "            dump = np.sort(-D_cosine, axis=1)\n",
    "            idx = np.argsort(-D_cosine, axis=1)\n",
    "            idx_new = idx[:, 0:k+1]\n",
    "            dump_new = -dump[:, 0:k+1]\n",
    "            G = np.zeros((n_samples*(k+1), 3))\n",
    "            G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n",
    "            G[:, 1] = np.ravel(idx_new, order='F')\n",
    "            G[:, 2] = np.ravel(dump_new, order='F')\n",
    "            # build the sparse affinity matrix W\n",
    "            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            bigger = np.transpose(W) > W\n",
    "            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "            return W\n",
    "\n",
    "    # choose supervised neighborMode\n",
    "    elif kwargs['neighbor_mode'] == 'supervised':\n",
    "        k = kwargs['k']\n",
    "        # get true labels and the number of classes\n",
    "        y = kwargs['y']\n",
    "        label = np.unique(y)\n",
    "        n_classes = np.unique(y).size\n",
    "        # construct the weight matrix W in a fisherScore way, W_ij = 1/n_l if yi = yj = l, otherwise W_ij = 0\n",
    "        if kwargs['fisher_score'] is True:\n",
    "            W = lil_matrix((n_samples, n_samples))\n",
    "            for i in range(n_classes):\n",
    "                class_idx = (y == label[i])\n",
    "                class_idx_all = (class_idx[:, np.newaxis] & class_idx[np.newaxis, :])\n",
    "                W[class_idx_all] = 1.0/np.sum(np.sum(class_idx))\n",
    "            return W\n",
    "\n",
    "        # construct the weight matrix W in a reliefF way, NH(x) and NM(x,y) denotes a set of k nearest\n",
    "        # points to x with the same class as x, a different class (the class y), respectively. W_ij = 1 if i = j;\n",
    "        # W_ij = 1/k if x_j \\in NH(x_i); W_ij = -1/(c-1)k if x_j \\in NM(x_i, y)\n",
    "        if kwargs['reliefF'] is True:\n",
    "            # when xj in NH(xi)\n",
    "            G = np.zeros((n_samples*(k+1), 3))\n",
    "            id_now = 0\n",
    "            for i in range(n_classes):\n",
    "                class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                D = pairwise_distances(X[class_idx, :])\n",
    "                D **= 2\n",
    "                idx = np.argsort(D, axis=1)\n",
    "                idx_new = idx[:, 0:k+1]\n",
    "                n_smp_class = (class_idx[idx_new[:]]).size\n",
    "                if len(class_idx) <= k:\n",
    "                    k = len(class_idx) - 1\n",
    "                G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n",
    "                G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n",
    "                G[id_now:n_smp_class+id_now, 2] = 1.0/k\n",
    "                id_now += n_smp_class\n",
    "            W1 = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            # when i = j, W_ij = 1\n",
    "            for i in range(n_samples):\n",
    "                W1[i, i] = 1\n",
    "            # when x_j in NM(x_i, y)\n",
    "            G = np.zeros((n_samples*k*(n_classes - 1), 3))\n",
    "            id_now = 0\n",
    "            for i in range(n_classes):\n",
    "                class_idx1 = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                X1 = X[class_idx1, :]\n",
    "                for j in range(n_classes):\n",
    "                    if label[j] != label[i]:\n",
    "                        class_idx2 = np.column_stack(np.where(y == label[j]))[:, 0]\n",
    "                        X2 = X[class_idx2, :]\n",
    "                        D = pairwise_distances(X1, X2)\n",
    "                        idx = np.argsort(D, axis=1)\n",
    "                        idx_new = idx[:, 0:k]\n",
    "                        n_smp_class = len(class_idx1)*k\n",
    "                        G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx1, (k, 1)).reshape(-1)\n",
    "                        G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx2[idx_new[:]], order='F')\n",
    "                        G[id_now:n_smp_class+id_now, 2] = -1.0/((n_classes-1)*k)\n",
    "                        id_now += n_smp_class\n",
    "            W2 = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            bigger = np.transpose(W2) > W2\n",
    "            W2 = W2 - W2.multiply(bigger) + np.transpose(W2).multiply(bigger)\n",
    "            W = W1 + W2\n",
    "            return W\n",
    "\n",
    "        if kwargs['weight_mode'] == 'binary':\n",
    "            if kwargs['metric'] == 'euclidean':\n",
    "                G = np.zeros((n_samples*(k+1), 3))\n",
    "                id_now = 0\n",
    "                for i in range(n_classes):\n",
    "                    class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                    # compute pairwise euclidean distances for instances in class i\n",
    "                    D = pairwise_distances(X[class_idx, :])\n",
    "                    D **= 2\n",
    "                    # sort the distance matrix D in ascending order for instances in class i\n",
    "                    idx = np.argsort(D, axis=1)\n",
    "                    idx_new = idx[:, 0:k+1]\n",
    "                    n_smp_class = len(class_idx)*(k+1)\n",
    "                    G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n",
    "                    G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n",
    "                    G[id_now:n_smp_class+id_now, 2] = 1\n",
    "                    id_now += n_smp_class\n",
    "                # build the sparse affinity matrix W\n",
    "                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "                bigger = np.transpose(W) > W\n",
    "                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "                return W\n",
    "\n",
    "            if kwargs['metric'] == 'cosine':\n",
    "                # normalize the data first\n",
    "                X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n",
    "                for i in range(n_samples):\n",
    "                    X[i, :] = X[i, :]/max(1e-12, X_normalized[i])\n",
    "                G = np.zeros((n_samples*(k+1), 3))\n",
    "                id_now = 0\n",
    "                for i in range(n_classes):\n",
    "                    class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                    # compute pairwise cosine distances for instances in class i\n",
    "                    D_cosine = np.dot(X[class_idx, :], np.transpose(X[class_idx, :]))\n",
    "                    # sort the distance matrix D in descending order for instances in class i\n",
    "                    idx = np.argsort(-D_cosine, axis=1)\n",
    "                    idx_new = idx[:, 0:k+1]\n",
    "                    n_smp_class = len(class_idx)*(k+1)\n",
    "                    G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n",
    "                    G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n",
    "                    G[id_now:n_smp_class+id_now, 2] = 1\n",
    "                    id_now += n_smp_class\n",
    "                # build the sparse affinity matrix W\n",
    "                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "                bigger = np.transpose(W) > W\n",
    "                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "                return W\n",
    "\n",
    "        elif kwargs['weight_mode'] == 'heat_kernel':\n",
    "            G = np.zeros((n_samples*(k+1), 3))\n",
    "            id_now = 0\n",
    "            for i in range(n_classes):\n",
    "                class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                # compute pairwise cosine distances for instances in class i\n",
    "                D = pairwise_distances(X[class_idx, :])\n",
    "                D **= 2\n",
    "                # sort the distance matrix D in ascending order for instances in class i\n",
    "                dump = np.sort(D, axis=1)\n",
    "                idx = np.argsort(D, axis=1)\n",
    "                idx_new = idx[:, 0:k+1]\n",
    "                dump_new = dump[:, 0:k+1]\n",
    "                t = kwargs['t']\n",
    "                # compute pairwise heat kernel distances for instances in class i\n",
    "                dump_heat_kernel = np.exp(-dump_new/(2*t*t))\n",
    "                n_smp_class = len(class_idx)*(k+1)\n",
    "                G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n",
    "                G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n",
    "                G[id_now:n_smp_class+id_now, 2] = np.ravel(dump_heat_kernel, order='F')\n",
    "                id_now += n_smp_class\n",
    "            # build the sparse affinity matrix W\n",
    "            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            bigger = np.transpose(W) > W\n",
    "            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "            return W\n",
    "\n",
    "        elif kwargs['weight_mode'] == 'cosine':\n",
    "            # normalize the data first\n",
    "            X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n",
    "            for i in range(n_samples):\n",
    "                X[i, :] = X[i, :]/max(1e-12, X_normalized[i])\n",
    "            G = np.zeros((n_samples*(k+1), 3))\n",
    "            id_now = 0\n",
    "            for i in range(n_classes):\n",
    "                class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                # compute pairwise cosine distances for instances in class i\n",
    "                D_cosine = np.dot(X[class_idx, :], np.transpose(X[class_idx, :]))\n",
    "                # sort the distance matrix D in descending order for instances in class i\n",
    "                dump = np.sort(-D_cosine, axis=1)\n",
    "                idx = np.argsort(-D_cosine, axis=1)\n",
    "                idx_new = idx[:, 0:k+1]\n",
    "                dump_new = -dump[:, 0:k+1]\n",
    "                n_smp_class = len(class_idx)*(k+1)\n",
    "                G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n",
    "                G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n",
    "                G[id_now:n_smp_class+id_now, 2] = np.ravel(dump_new, order='F')\n",
    "                id_now += n_smp_class\n",
    "            # build the sparse affinity matrix W\n",
    "            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            bigger = np.transpose(W) > W\n",
    "            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "            return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T11:49:38.513263Z",
     "start_time": "2022-12-07T11:49:38.477365Z"
    },
    "code_folding": [
     0,
     18,
     44,
     52,
     58,
     64,
     70
    ]
   },
   "outputs": [],
   "source": [
    "#函数\n",
    "import numpy as np\n",
    "from scipy.sparse import *\n",
    "res = pd.DataFrame(0,columns=[\"spnsi\",\"speci\",\"GM\",\"f1\",\"AUC\"],index=dataList)\n",
    "model = XGBClassifier(max_depth=15,\n",
    "                      learning_rate=0.01,\n",
    "                      n_estimators=1000,\n",
    "                      min_child_weight=5,\n",
    "                      max_delta_step=0,\n",
    "                      subsample=0.8,\n",
    "                      colsample_bytree=0.8,\n",
    "                      reg_alpha=0.1,\n",
    "                      reg_lambda=0.1,\n",
    "                      scale_pos_weight=0.8,\n",
    "                      silent=True,\n",
    "                      objective='binary:logistic',\n",
    "                      eval_metric='error',\n",
    "                      gamma=0)\n",
    "def fisher_score(X, y):\n",
    "\n",
    "    # Construct weight matrix W in a fisherScore way\n",
    "    kwargs = {\"neighbor_mode\": \"supervised\", \"fisher_score\": True, 'y': y}\n",
    "    W = construct_W(X, **kwargs)\n",
    "\n",
    "    # build the diagonal D matrix from affinity matrix W\n",
    "    D = np.array(W.sum(axis=1))\n",
    "    L = W\n",
    "    tmp = np.dot(np.transpose(D), X)\n",
    "    D = diags(np.transpose(D), [0])\n",
    "    Xt = np.transpose(X)\n",
    "    t1 = np.transpose(np.dot(Xt, D.todense()))\n",
    "    t2 = np.transpose(np.dot(Xt, L.todense()))\n",
    "    # compute the numerator of Lr\n",
    "    D_prime = np.sum(np.multiply(t1, X), 0) - np.multiply(tmp, tmp)/D.sum()\n",
    "    # compute the denominator of Lr\n",
    "    L_prime = np.sum(np.multiply(t2, X), 0) - np.multiply(tmp, tmp)/D.sum()\n",
    "    # avoid the denominator of Lr to be 0\n",
    "    D_prime[D_prime < 1e-12] = 10000\n",
    "    lap_score = 1 - np.array(np.multiply(L_prime, 1/D_prime))[0, :]\n",
    "\n",
    "    # compute fisher score from laplacian score, where fisher_score = 1/lap_score - 1\n",
    "    score = 1.0/lap_score - 1\n",
    "    return np.transpose(score)\n",
    "\n",
    "def feature_ranking(score):\n",
    "    \"\"\"\n",
    "    Rank features in descending order according to fisher score, the larger the fisher score, the more important the\n",
    "    feature is\n",
    "    \"\"\"\n",
    "    idx = np.argsort(score, 0)\n",
    "    return idx[::-1]\n",
    "\n",
    "def GM_score(x,y): #计算GM 传入numpy\n",
    "    cm1 = confusion_matrix(x,y)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    return sensitivity1,specificity1\n",
    "\n",
    "def getAncestors(feat):\n",
    "    global relativeList\n",
    "    for relative in DAG.columns:\n",
    "        if(DAG.loc[relative,feat] == 1):\n",
    "            relativeList.append(relative)\n",
    "            getAncestors(relative)\n",
    "def getDescendants(feat):\n",
    "    global relativeList\n",
    "    for relative in DAG.columns:\n",
    "        if(DAG.loc[feat,relative] == 1):\n",
    "            relativeList.append(relative)\n",
    "            getAncestors(relative)\n",
    "def get(feat):\n",
    "    global relativeList\n",
    "    corr = []\n",
    "    #先找到当前节点的所有祖先或后代节点\n",
    "    relativeList = []\n",
    "    getDescendants(feat)\n",
    "    getAncestors(feat)\n",
    "    relativeSet = set(relativeList)\n",
    "    for relative in relativeSet:\n",
    "        corr.append(SUMetrics.loc[relative,feat])\n",
    "    #print(corr)\n",
    "    if(corr == []):\n",
    "        return 0\n",
    "    else:\n",
    "        return np.mean(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T11:59:29.184957Z",
     "start_time": "2022-12-07T11:59:29.165038Z"
    }
   },
   "outputs": [],
   "source": [
    "dataList = [\"MM/MM-BP.csv\",\"MM/MM-CC.csv\",\"MM/MM-MF.csv\",\"MM/MM-BPCC.csv\",\n",
    "            \"MM/MM-BPMF.csv\",\"MM/MM-CCMF.csv\",\"MM/MM-BPCCMF.csv\",\n",
    "           \"CE/CE-BP.csv\",\"CE/CE-CC.csv\",\"CE/CE-MF.csv\",\"CE/CE-BPCC.csv\",\n",
    "            \"CE/CE-BPMF.csv\",\"CE/CE-CCMF.csv\",\"CE/CE-BPCCMF.csv\",\n",
    "           \"DM/DM-BP.csv\",\"DM/DM-CC.csv\",\"DM/DM-MF.csv\",\"DM/DM-BPCC.csv\",\n",
    "            \"DM/DM-BPMF.csv\",\"DM/DM-CCMF.csv\",\"DM/DM-BPCCMF.csv\",\n",
    "           \"SC/SC-BP.csv\",\"SC/SC-CC.csv\",\"SC/SC-MF.csv\",\"SC/SC-BPCC.csv\",\n",
    "            \"SC/SC-BPMF.csv\",\"SC/SC-CCMF.csv\",\"SC/SC-BPCCMF.csv\"]\n",
    "dataList = [\"SportsTweetst.csv\",\"SportsTweetsc.csv\",\"NYDailyheadings.csv\",\"Stumbleupon.csv\",\"Cities.csv\"]\n",
    "dataName = \"Stumbleupon.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T12:03:32.808832Z",
     "start_time": "2022-12-07T12:03:12.230407Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#加载数据 预处理\n",
    "time_start = time.time()  # 记录开始时间\n",
    "#data = pd.read_csv(\"../data/real_world_datasets/Datasets/\"+dataName,index_col=None)\n",
    "data = pd.read_csv(\"../data/real_world_datasets/Datasets/\"+dataName,index_col=None,encoding='gbk')#读取stu文件\n",
    "if(dataName == \"Cities.csv\"):\n",
    "    data[\"longevity influence\"] = data[\"longevity influence\"].apply(lambda x : 1 if x > 1.751738e+09 else 0)\n",
    "label = data[\"longevity influence\"]\n",
    "#删除无用列\n",
    "data = data.drop([\"longevity influence\"],axis=1)\n",
    "#过滤低纬度特征\n",
    "for c in data.columns:\n",
    "    data[c] = data[c].astype('int')\n",
    "    if(data[c].sum() < 3):\n",
    "        data.drop(c,axis=1,inplace=True)\n",
    "# DAG = pd.read_csv(\"../data/real_world_datasets/Hierarchy/\"+dataName,index_col=0)\n",
    "# DAG = DAG.loc[data.columns,data.columns]\n",
    "# labels_pred=data.values.tolist()\n",
    "# labels_true=label.values.tolist()\n",
    "#读取计算结果\n",
    "hierarchicalR = pd.read_csv(\"../\"+dataName,index_col=0)\n",
    "# Relevance = pd.Series(fisher_score(labels_pred,labels_true),index=data.columns)\n",
    "#α惩罚项\n",
    "# SUMetrics = pd.DataFrame(0,columns=data.columns,index=data.columns)\n",
    "# for c1 in data.columns:\n",
    "#     print(c1)\n",
    "#     for c2 in data.columns:\n",
    "#         if(c1 != c2):\n",
    "#             SUMetrics.loc[c1,c2] = metrics.normalized_mutual_info_score(data.loc[:,c1],data.loc[:,c2])\n",
    "\n",
    "# relativeList = []\n",
    "# global relativeList\n",
    "# avgCorr = pd.Series(0,index=data.columns)\n",
    "# for c in data.columns:\n",
    "#     print(1)\n",
    "#     avgCorr.loc[c] = get(c)\n",
    "\n",
    "#β惩罚项\n",
    "\n",
    "# relativeList = []\n",
    "# global relativeList\n",
    "# countAncestors = pd.Series(0,index=data.columns)\n",
    "# for c in data.columns[4:]:\n",
    "#     print(1)\n",
    "#     relativeList = []\n",
    "#     getAncestors(c) \n",
    "#     countAncestors.loc[c] = len(set(relativeList))\n",
    "\n",
    "#归一化\n",
    "# Relevance = (Relevance - Relevance.min()) / (Relevance.max() - Relevance.min())\n",
    "# avgCorr = (avgCorr - avgCorr.min()) / (avgCorr.max() - avgCorr.min())\n",
    "# countAncestors = (countAncestors - countAncestors.min()) / (countAncestors.max() - countAncestors.min())\n",
    "# #带层次冗余惩罚的相关性系数\n",
    "# alpha = 0.1\n",
    "# beta = 0.2\n",
    "#hierarchicalR = Relevance - alpha * avgCorr - beta * countAncestors\n",
    "#hierarchicalR = hierarchicalR.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T12:10:13.133328Z",
     "start_time": "2022-12-07T12:03:46.622331Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:03:46] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-03de431ba26204c4d-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0\n",
      " 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0\n",
      " 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0\n",
      " 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1\n",
      " 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1\n",
      " 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0\n",
      " 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1\n",
      " 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0\n",
      " 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0\n",
      " 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1\n",
      " 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1\n",
      " 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0\n",
      " 1 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0\n",
      " 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1\n",
      " 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0\n",
      " 1 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0\n",
      " 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 1 0 0\n",
      " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1\n",
      " 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1\n",
      " 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0\n",
      " 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0\n",
      " 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1\n",
      " 1 1 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
      " 0 1 0 1 0 0 1 1 1 0 0 0]\n",
      "[20:05:05] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-03de431ba26204c4d-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1\n",
      " 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0\n",
      " 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 1\n",
      " 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0\n",
      " 0 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0\n",
      " 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1\n",
      " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0\n",
      " 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1\n",
      " 1 1 0 0 0 0 1 0 0 0 0 0]\n",
      "[1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1\n",
      " 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0\n",
      " 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0\n",
      " 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1\n",
      " 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1\n",
      " 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0\n",
      " 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1\n",
      " 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0\n",
      " 0 1 1 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0\n",
      " 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1\n",
      " 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0\n",
      " 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 1 1 0 0\n",
      " 1 1 0 0 0 1 1 1 0 0 1 0]\n",
      "[20:06:21] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-03de431ba26204c4d-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0\n",
      " 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0\n",
      " 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0\n",
      " 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0\n",
      " 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1\n",
      " 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0\n",
      " 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
      " 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0\n",
      " 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0\n",
      " 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0\n",
      " 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0\n",
      " 1 0 0 0 1 1 1 1 0 0 0 1]\n",
      "[0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1\n",
      " 0 0 0 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0\n",
      " 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0\n",
      " 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0\n",
      " 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0\n",
      " 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0\n",
      " 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1\n",
      " 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
      " 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0\n",
      " 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0\n",
      " 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1\n",
      " 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0\n",
      " 0 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0\n",
      " 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0\n",
      " 1 0 0 1 0 0 1 1 0 1 0 1]\n",
      "[20:07:36] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-03de431ba26204c4d-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0\n",
      " 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
      " 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0\n",
      " 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
      " 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0\n",
      " 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0\n",
      " 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0\n",
      " 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 1 0\n",
      " 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
      " 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 1 0 0 1 0 0 0 0 0]\n",
      "[0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
      " 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 0\n",
      " 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0\n",
      " 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0\n",
      " 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0\n",
      " 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0\n",
      " 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0\n",
      " 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 0 0 0\n",
      " 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1\n",
      " 0 1 0 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0\n",
      " 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0\n",
      " 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0\n",
      " 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0\n",
      " 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1\n",
      " 1 1 0 1 0 0 1 0 0 0 1 0]\n",
      "[20:08:53] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-03de431ba26204c4d-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0\n",
      " 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0\n",
      " 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0\n",
      " 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0\n",
      " 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 0 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0\n",
      " 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0\n",
      " 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0\n",
      " 1 0 1 1 0 0 1 0 1 0 0 0]\n",
      "[1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1\n",
      " 0 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0\n",
      " 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0\n",
      " 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 1 0 1\n",
      " 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
      " 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1\n",
      " 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1\n",
      " 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0\n",
      " 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
      " 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0\n",
      " 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1\n",
      " 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0\n",
      " 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0\n",
      " 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0\n",
      " 1 0 0 1 1 0 1 1 1 0 1 0]\n",
      "0.7233018553707905\n",
      "0.7843240913764034\n",
      "GM\n",
      "75.3\n",
      "f1\n",
      "68.2\n",
      "AUC\n",
      "75.4\n",
      "420.8813147544861\n"
     ]
    }
   ],
   "source": [
    "#执行\n",
    "num = int(data.shape[1]*0.7)\n",
    "sensitivity = []\n",
    "specificity = []\n",
    "F1 = []\n",
    "AUC = []\n",
    "numList = []\n",
    "#10折交叉验证\n",
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "for train_index ,test_index in kf.split(data):\n",
    "    trainData = data.iloc[train_index,:]\n",
    "    testData = data.iloc[test_index,:]\n",
    "    Y_train = label.values[train_index] #用于训练模型\n",
    "    Y_test = label.values[test_index]#用于交叉验证\n",
    "\n",
    "    selectFeatures = hierarchicalR[:num].index#CFS\n",
    "    #selectFeatures = data.columns\n",
    "\n",
    "    X_train = trainData.loc[:,selectFeatures]\n",
    "    X_test = testData.loc[:,selectFeatures]\n",
    "    model.fit(X_train,Y_train) #xgb\n",
    "    predictList = model.predict(X_test)\n",
    "#     gnb = GaussianNB()\n",
    "#     predictList = gnb.fit(X_train, Y_train).predict(X_test)\n",
    "#     from sklearn.svm import SVC\n",
    "#     clf = make_pipeline(StandardScaler(), SVC(gamma=0.5))\n",
    "#     clf.fit(X_train, Y_train)\n",
    "#     predictList = clf.predict(X_test)\n",
    "    print(predictList)\n",
    "    print(Y_test)\n",
    "    sensi,speci= GM_score(predictList,Y_test)\n",
    "    sensitivity.append(sensi)\n",
    "    specificity.append(speci)\n",
    "    F1.append(f1_score(np.array(predictList),Y_test))\n",
    "    try:\n",
    "        AUC.append(roc_auc_score(np.array(predictList),Y_test))\n",
    "    except ValueError:\n",
    "        pass\n",
    "#standard\n",
    "a = np.nanmean(sensitivity)\n",
    "b = np.nanmean(specificity)\n",
    "print(a)\n",
    "print(b)\n",
    "print(\"GM\")\n",
    "print(round(math.sqrt(a*b)*100,1))\n",
    "print(\"f1\")\n",
    "print(round(np.nanmean(F1)*100,1))\n",
    "print(\"AUC\") \n",
    "print(round(np.nanmean(AUC)*100,1))\n",
    "res.loc[dataName,\"spnsi\"] = round(a*100,1)\n",
    "res.loc[dataName,\"speci\"] = round(b*100,1)\n",
    "res.loc[dataName,\"GM\"] = round(math.sqrt(a*b)*100,1)\n",
    "res.loc[dataName,\"f1\"] = round(np.nanmean(F1)*100,1)\n",
    "res.loc[dataName,\"AUC\"] = round(np.nanmean(AUC)*100,1)\n",
    "\n",
    "time_end = time.time()  # 记录结束时间\n",
    "time_sum = time_end - time_start  # 计算的时间差为程序的执行时间，单位为秒/s\n",
    "print(time_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T13:12:17.594399Z",
     "start_time": "2022-12-07T13:12:17.571459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spnsi</th>\n",
       "      <th>speci</th>\n",
       "      <th>GM</th>\n",
       "      <th>f1</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SportsTweetst.csv</th>\n",
       "      <td>70.0</td>\n",
       "      <td>91.2</td>\n",
       "      <td>79.9</td>\n",
       "      <td>63.0</td>\n",
       "      <td>80.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SportsTweetsc.csv</th>\n",
       "      <td>81.4</td>\n",
       "      <td>95.1</td>\n",
       "      <td>88.0</td>\n",
       "      <td>82.1</td>\n",
       "      <td>88.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NYDailyheadings.csv</th>\n",
       "      <td>49.6</td>\n",
       "      <td>61.4</td>\n",
       "      <td>55.2</td>\n",
       "      <td>63.4</td>\n",
       "      <td>55.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stumbleupon.csv</th>\n",
       "      <td>72.3</td>\n",
       "      <td>78.4</td>\n",
       "      <td>75.3</td>\n",
       "      <td>68.2</td>\n",
       "      <td>75.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cities.csv</th>\n",
       "      <td>33.3</td>\n",
       "      <td>82.6</td>\n",
       "      <td>52.5</td>\n",
       "      <td>88.7</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     spnsi  speci    GM    f1   AUC\n",
       "SportsTweetst.csv     70.0   91.2  79.9  63.0  80.6\n",
       "SportsTweetsc.csv     81.4   95.1  88.0  82.1  88.3\n",
       "NYDailyheadings.csv   49.6   61.4  55.2  63.4  55.5\n",
       "Stumbleupon.csv       72.3   78.4  75.3  68.2  75.4\n",
       "Cities.csv            33.3   82.6  52.5  88.7  58.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.to_csv(\"../res.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
